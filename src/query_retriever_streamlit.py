import os
import streamlit as st
from langchain import hub
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import FAISS
from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# OpenAI API ì„¤ì •
api_key = os.getenv("OPENAI_API_KEY")
os.environ["OPENAI_API_KEY"] = api_key
if not api_key:
    raise ValueError("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")

# ì„ë² ë”© ëª¨ë¸ ìƒì„± - text-embedding-3-small ì‚¬ìš©
embedding_model = OpenAIEmbeddings(model="text-embedding-3-small")

# ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ (allow_dangerous_deserialization ì¸ìë¥¼ ì¶”ê°€)
vectorstore = FAISS.load_local("vdb/faiss_index", embeddings=embedding_model, allow_dangerous_deserialization=True)

# lambda_multê°€ í¬ë©´ ì •í™•ë„ í–¥ìƒ, ì‘ìœ¼ë©´ ë‹¤ì–‘ì„± í–¥ìƒ
retriever = vectorstore.as_retriever(search_type='mmr', search_kwargs={'k': 5, 'fetch_k': 10, 'lambda_mult': 0.9})

# RAG êµ¬ì„± ìš”ì†Œ ì„¤ì •
prompt = hub.pull("rlm/rag-prompt")
llm = ChatOpenAI(model_name="gpt-4o", temperature=0.5)
rag_chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

# Streamlit ì•± ì„¤ì •
st.set_page_config(page_title="RAG ê¸°ë°˜ ì±—ë´‡", page_icon="ğŸ¤–", layout="wide")

st.title("ğŸ“„ RAG ê¸°ë°˜ ì„¸ë¬´ì‚¬ ì±—ë´‡")
st.write("ì§ˆë¬¸ì„ ì…ë ¥í•˜ë©´ ê´€ë ¨ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ê³  ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.")

# ì‚¬ìš©ì ì…ë ¥ í¼
with st.form("chat_form"):
    question = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:", placeholder="ì˜ˆ: ëŒ€í•™ì›ìƒì¸ ë°°ìš°ìê°€ 2024ë…„ 6ì›”ì— ì—°êµ¬ìš©ì—­ë¹„ë¡œ 500ë§Œì›ì„ ë°›ì€ ê²½ìš° ë°°ìš°ìê³µì œê°€ ê°€ëŠ¥í•´?")
    submit_button = st.form_submit_button(label="ì§ˆë¬¸í•˜ê¸°")

if submit_button and question:
    # ë¬¸ì„œ ê²€ìƒ‰
    retrieved_documents = retriever.invoke(question)

    # ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ì„ ê²½ìš° ì²˜ë¦¬
    if not retrieved_documents:
        st.warning("ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    else:
        # RAGë¥¼ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µ ìƒì„±
        with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
            response = rag_chain.invoke(question)
            
        # ì‘ë‹µ ì¶œë ¥
        st.subheader("ğŸ’¡ ìƒì„±ëœ ë‹µë³€")
        st.write(response)
        
        # ë¦¬íŠ¸ë¦¬ë²„ëœ ë¬¸ì„œë¥¼ Expandë¡œ ì¶œë ¥
        st.subheader("ğŸ” ì°¸ì¡°í•œ ë¬¸ì„œ")
        for idx, doc in enumerate(retrieved_documents, 1):
            with st.expander(f"ë¬¸ì„œ {idx}: {doc.metadata.get('ì œëª©', 'ì œëª© ì—†ìŒ')}"):
                st.write(f"**ì œëª©:** {doc.metadata.get('ì œëª©', 'ì—†ìŒ')}")
                st.write(f"**ë³¸ë¬¸:** {doc.page_content}")
                st.write(f"**ì¶œì²˜:** {doc.metadata.get('source', 'ì¶œì²˜ ì—†ìŒ')}")
